{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3077ca49-b9e7-4fb0-a3a1-bd7c25ce26a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "import torch\n",
    "\n",
    "graph_tokenizer = tokenizer.GraphTokenizer(torch.load(\"dictionary.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5230b142-7b8d-43d8-bbcf-09e224afab87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 500/500 [00:00<00:00, 963.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'graph1': BlendData(x=[254, 9], edge_index=[2, 476], edge_attr=[476, 3], blend_batch=[28], mol_batch=[254]),\n",
       " 'graph2': BlendData(x=[239, 9], edge_index=[2, 452], edge_attr=[452, 3], blend_batch=[28], mol_batch=[239]),\n",
       " 'y': tensor(0.5769)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import data\n",
    "import torch_geometric as tg\n",
    "\n",
    "all_data = []\n",
    "with h5py.File('Data/train.h5', 'r') as f:\n",
    "    for label in tqdm.tqdm(f.keys()):\n",
    "        group = f[label]\n",
    "        graph1 = data.read_graph(group['graph1'])\n",
    "        graph2 = data.read_graph(group['graph2'])\n",
    "        # Index using () for scalar dataset\n",
    "        y = group[\"y\"][()]\n",
    "        all_data.append({\"graph1\":graph1,\"graph2\":graph2,\"y\":torch.tensor(y)})\n",
    "\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3cba35-3131-4003-badd-948fac53d66e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlendData(x=[254], edge_index=[2, 476], edge_attr=[476], blend_batch=[28], mol_batch=[254])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_tokenizer.tokenize(all_data[0][\"graph1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e69607-d2d3-4061-9e13-bd2bc6401ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sequential.forward() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(DataLoader([all_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph1\u001b[39m\u001b[38;5;124m\"\u001b[39m],all_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph2\u001b[39m\u001b[38;5;124m\"\u001b[39m]],batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(agg(batch\u001b[38;5;241m.\u001b[39mx,batch)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(agg(all_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mx,all_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/rdenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rdenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dream/aggregate.py:51\u001b[0m, in \u001b[0;36mBlendAggregator.forward\u001b[0;34m(self, x, graph)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, graph):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_two_stage:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(x,graph)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(graph, tg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBatch):\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(x, graph\u001b[38;5;241m.\u001b[39mbatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/rdenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rdenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dream/aggregate.py:30\u001b[0m, in \u001b[0;36mTwoStageAggregator.forward\u001b[0;34m(self, x, graph)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, graph):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Aggregation over atoms in a molecule\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmol_readout(x, index\u001b[38;5;241m=\u001b[39mgraph\u001b[38;5;241m.\u001b[39mmol_batch)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Aggregatation over molecules in a blend\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblend_readout(x, index\u001b[38;5;241m=\u001b[39mgraph\u001b[38;5;241m.\u001b[39mblend_batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/rdenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rdenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Sequential.forward() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "import aggregate\n",
    "\n",
    "agg = aggregate.BlendAggregator(True,9,1,1,0)\n",
    "from torch_geometric.loader import DataLoader\n",
    "batch = next(iter(DataLoader([all_data[0][\"graph1\"],all_data[0][\"graph2\"]],batch_size=2)))\n",
    "print(agg(batch.x,batch).shape)\n",
    "print(agg(all_data[0][\"graph1\"].x,all_data[0][\"graph1\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0f70a-86f6-48ee-bbf6-b91316daf606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import aggregate\n",
    "\n",
    "agg = aggregate.BlendAggregator(False,9,1,1,0)\n",
    "from torch_geometric.loader import DataLoader\n",
    "batch = next(iter(DataLoader([all_data[0][\"graph1\"],all_data[0][\"graph2\"]],batch_size=2)))\n",
    "print(agg(batch.x,batch).shape)\n",
    "print(agg(all_data[0][\"graph1\"].x,all_data[0][\"graph1\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f2dcb-0f2a-4b1a-a6a0-2e1cd0eca8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mpnn\n",
    "\n",
    "config = mpnn.Config(node_out_feats=16,\n",
    "                 edge_hidden_feats=16, num_step_message_passing=3)\n",
    "model = mpnn.from_config(config,node_in_feats=9, edge_in_feats=3,dropout=.1, do_edge_update=False, act_mode=\"relu\", aggr_mode=\"mean\")\n",
    "exmpl = all_data[0][\"graph1\"]\n",
    "model(exmpl,exmpl.x,exmpl.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe261d8c-514c-42e0-a9ca-806264b5b282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mpnn\n",
    "\n",
    "config = mpnn.Config(node_out_feats=16,\n",
    "                 edge_hidden_feats=16, num_step_message_passing=3)\n",
    "model = mpnn.from_config(config,node_in_feats=9, edge_in_feats=3,dropout=.1, do_edge_update=True, act_mode=\"silu\", aggr_mode=\"mean\")\n",
    "exmpl = all_data[0][\"graph1\"]\n",
    "model(exmpl,exmpl.x,exmpl.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bc1dd-5210-45c8-8bb1-a8615e7fe1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.readout_counts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40fa2f-d978-48ba-bd80-aa02a7061ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import encoder\n",
    "import torch\n",
    "\n",
    "mpnn_configs = [mpnn.Config(node_out_feats=16,\n",
    "                 edge_hidden_feats=8, num_step_message_passing=5), mpnn.Config(node_out_feats=64,\n",
    "                 edge_hidden_feats=32, num_step_message_passing=3), mpnn.Config(node_out_feats=128,\n",
    "                 edge_hidden_feats=64, num_step_message_passing=1)]\n",
    "config = {\"mpnn_configs\":mpnn_configs, \"do_two_stage\":True, \"do_edge_update\":True, \"embedding_dim_x\":32, \"embedding_dim_edge_attr\": 64, \"do_edge_update\":True, \"num_sabs\":8,\"dropout\":0.1, \"heads\":8, \"warmup\":.05, \"lr\": 1e-3, \"weight_decay\":.01, \"betas\":(.99,.999), \"act_mode\":\"silu\", \"aggr_mode\":\"mean\"}\n",
    "ex_model = encoder.Encoder(graph_tokenizer=None,**config)\n",
    "ex_model(exmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d0d07-cfe8-4fbf-bcfb-bc9802aa1dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpnn_configs = [mpnn.Config(node_out_feats=64,\n",
    "                 edge_hidden_feats=32, num_step_message_passing=3)]\n",
    "config = {\"mpnn_configs\":mpnn_configs,  \"do_two_stage\":False, \"embedding_dim_x\":32, \"embedding_dim_edge_attr\": 64, \"do_edge_update\":False, \"num_sabs\":8,\"dropout\":0.1, \"heads\":8, \"warmup\":.05, \"lr\": 1e-3, \"weight_decay\":.01, \"betas\":(.99,.999), \"act_mode\":\"gelu\",\"aggr_mode\":\"max\"}\n",
    "ex_model = encoder.Encoder(graph_tokenizer=graph_tokenizer,**config)\n",
    "exmpl_tokenized_graph = graph_tokenizer.tokenize(all_data[0][\"graph1\"])\n",
    "ex_model(exmpl_tokenized_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70f470-6ee0-4b90-8375-46747f574fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossEncoder(torch.nn.Module):\n",
    "    def __init__(self,encoder, do_cosine_similarity, **kwargs):\n",
    "        super(CrossEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.do_cosine_similarity = do_cosine_similarity\n",
    "        if not self.do_cosine_similarity:\n",
    "          self.readout = torch.nn.Linear(self.encoder.readout.in_channels*2,1)\n",
    "\n",
    "    def forward(self,graph1, graph2):\n",
    "      embed1 = self.encoder(graph1)\n",
    "      embed2 = self.encoder(graph2)\n",
    "\n",
    "      if self.do_cosine_similarity:\n",
    "        return torch.nn.functional.cosine_similarity(embed1,embed2)\n",
    "\n",
    "      return torch.nn.functional.sigmoid(self.readout(torch.cat([embed1,embed2],dim=-1))).squeeze(dim=-1)\n",
    "\n",
    "m = CrossEncoder(ex_model,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
